# -*- coding: utf-8 -*-
"""Submit-your-final-Jupyter-Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bt2WuO5BSdYvOYNw7uoUe2R8FgMzqlNL

**Welcome to my experiment on the GDP-Based Recession Indicator Index dataset!** 

Specifically, we will be analyzing the following objective:

**Problem to solve**: *How can we model this data in order to predict future periods of economic recession in the United States?* 

The techniques we will be using for this project are as follows:

*   Time Series Analysis with ARIMA models
*   Deep Learning with ANN models

Thank you for taking the time to view this data with me - now let's explore it together!
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries/packages:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy
from datetime import datetime
import datetime as dt
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

"""**Dataset:** GDP-Based Recession Indicator Index *(JHGDPBRINDX)*

**Source Page with Description:** https://fred.stlouisfed.org/series/JHGDPBRINDX

______________________________

**Citation:** Hamilton, James, GDP-Based Recession Indicator Index [JHGDPBRINDX], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/JHGDPBRINDX, September 15, 2022.
"""

df = pd.read_csv("https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=off&txtcolor=%23444444&ts=12&tts=12&width=1168&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=JHGDPBRINDX&scale=left&cosd=1967-10-01&coed=2022-01-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2022-09-17&revision_date=2022-09-17&nd=1967-10-01")

"""# **Exploratory Data Analysis**"""

df.info()
df.head(30)

df.describe()

# Finding start and end dates of the data:
print(df.min())
print(df.max())

# Remove `NA` values
df.dropna(inplace=True)

"""*Initial View of the Data*

*   First, by date.
*   Then, by marginal differences over time.


"""

df.index = pd.to_datetime(df.DATE)
df.plot()
plt.title('GDP-Based Recession Indicator Index')
plt.show()

df['diff'] = df.JHGDPBRINDX - df.JHGDPBRINDX.shift()
df.plot(y='diff')
plt.title('GDP-Based Recession Indicator Index')
plt.show()

"""After viewing the properties of the dataset and the initial visualizations of the data after cleaning it, it appears that there may be a trend of seasonality describing the points at which the United States has historically entered and exited economic recessions. As a result, it may be valuable to define the data in the following ways as we analyze it further:

*   Training several ARIMA models in order to reflect the analysis of seasons of recession in the data and attempt to minimize residuals without overfitting.
*   Training an ANN program to predict future data based on the three different categories of time present in the data: 
  * The quarters at which the United States' economy enters a recession, 
  * The quarters at which it exits a recession, 
  * And the duration of quarters in between each point of entering and exiting recessions.

# **Time Series Analysis**
"""

# import libraries for time series analysis with PACF and ARIMA:
from statsmodels.tsa.stattools import pacf
from statsmodels.tsa.arima_model import ARIMA

# PACF for the initial time series:
x_acf = pd.DataFrame(pacf(df.JHGDPBRINDX))
x_acf.plot(kind='bar')
plt.title("GDP-Based Recession PACF")
plt.legend(['autocorrelation'])

# PACF for the differenced time series:
df['diff'].astype('float')
diff_acf = pd.DataFrame(pacf(df['diff'][1:]))
diff_acf.plot(kind='bar')
plt.title("GDP-Based Recession Diff PACF")
plt.legend(['difference'])

"""According to the consistent rise and fall of values in the plots showing the autorrelation function for the GDP-Based Recession Indicator Index and the differenced time series, it is again apparent that the United States' economy tends to enter and exit recessions in a seasonal pattern.
 * Considering the strong autocorrelation at values of `1` and `2` for both representations of the data, it would be reasonable to set parameter *p* to `1` and `2` in the development of ARIMA models next.
"""

model_010 = ARIMA(df.JHGDPBRINDX, order=(0,1,0))
model_010_fit = model_010.fit()
print(model_010_fit.summary())
print('Residuals Description')
print(model_010_fit.resid.describe())

model_100 = ARIMA(df.JHGDPBRINDX, order=(1,0,0))
model_100_fit = model_100.fit()
print(model_100_fit.summary())
print('Residuals Description')
print(model_100_fit.resid.describe())

model_101 = ARIMA(df.JHGDPBRINDX, order=(1,0,1))
model_101_fit = model_101.fit()
print(model_101_fit.summary())
print('Residuals Description')
print(model_101_fit.resid.describe())

model_110 = ARIMA(df.JHGDPBRINDX, order=(1,1,0))
model_110_fit = model_110.fit()
print(model_110_fit.summary())
print('Residuals Description')
print(model_110_fit.resid.describe())

model_111 = ARIMA(df.JHGDPBRINDX, order=(1,1,1))
model_111_fit = model_111.fit()
print(model_111_fit.summary())
print('Residuals Description')
print(model_111_fit.resid.describe())

model_200 = ARIMA(df.JHGDPBRINDX, order=(2,0,0))
model_200_fit = model_200.fit()
print(model_200_fit.summary())
print('Residuals Description')
print(model_200_fit.resid.describe())

model_201 = ARIMA(df.JHGDPBRINDX, order=(2,0,1))
model_201_fit = model_201.fit()
print(model_201_fit.summary())
print('Residuals Description')
print(model_201_fit.resid.describe())

model_210 = ARIMA(df.JHGDPBRINDX, order=(2,1,0))
model_210_fit = model_210.fit()
print(model_210_fit.summary())
print('Residuals Description')
print(model_210_fit.resid.describe())

model_211 = ARIMA(df.JHGDPBRINDX, order=(2,1,1))
model_211_fit = model_211.fit()
print(model_211_fit.summary())
print('Residuals Description')
print(model_211_fit.resid.describe())

model_212 = ARIMA(df.JHGDPBRINDX, order=(2,1,2))
model_212_fit = model_212.fit()
print(model_212_fit.summary())
print('Residuals Description')
print(model_212_fit.resid.describe())

"""*Evaluating the ARIMA models:*

**Log-likelihood (probability)**
* ARIMA(2,1,2) has the highest log-likelihood value.
  * It has the log-likelihood probability closest to zero, which may stem from the actual likelihood probability closest to 1.0, in this case.
    * In other words, the probability that this model predicts new data in a manner that closely resembles the actual data is higher than the other ARIMA models built here.

**Akaike information criterion (AIC)**
* ARIMA(2,1,2) has the lowest AIC score.
  * In effect, setting these parameters thus minimizes the prediction error of this statistical model developed from the data.

**Initial Conclusion**
* ARIMA(2,1,2) appears to be the best-fit model that exhibits a concentration on the evolution of the data over time.
"""

residuals = pd.DataFrame(model_212_fit.resid)

residuals.plot(legend=False)
plt.title('Time Series of Residuals')

residuals.hist(bins=20)
plt.title('Histogram of Residuals')

"""**Analysis of Residuals:**

After viewing these plots of the residuals, two reasonable assumptions are evident to deduce about the data:

* Firstly, a trend of periodicity is clearly present in the Time Series of Residuals plot. 
  * This is likely indicating validation of seasonality in the data, as it is a feature of time series visualization that persisted prior to and throughout the ARIMA modeling process.

* Additionally, the histogram of residuals resembles a normal distribution that is very-slightly left-skewed. 
  * There is a rather long tail on the lower end, but that is quite a small number of observations that are likely outliers (as exhibited by the intermittent bumps that appear towards the end of the tail).
  * This aspect confirms the reliability of the ARIMA model, as it accounts for the assumption made about errors in linear modeling.

Next, a formulation of an artificial neural network (ANN) model may be valuable for learning when to predict future GDP-based indications of economic recession in consideration of this multiple-times-apparent instance of a serial trend in the data.

# **Deep Learning**
"""

# import libraries for deep learning using TensorFlow and Keras:
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

from sklearn.model_selection import train_test_split

df.shape

# Using multiple conditions to train a variable for y so that:
#   1 means "entered a recession 
#     (or still in a recession, if above most recent score of entering and values still have not gone below that score)"
#   0.5 means "either heading-towards or moving-out-of a recession"
#     (tracking intermediate periods in order to avoid inaccurately categorizing them with periods of recession or non-recession)
#   0 means '"exited a recession 
#     (or still out of a recession, if below most recent score of exiting and values still have not gone above that score)"

df['trained_index'] = np.where(df.JHGDPBRINDX > 67.0, 1, np.where(df.JHGDPBRINDX < 33.0, 0, df.JHGDPBRINDX.values))
df.trained_index = np.where(df.trained_index > 32.0, 0.5, df.trained_index.values)
df.trained_index.head(25)

X = df.JHGDPBRINDX
y = df.trained_index

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.25, random_state = 3)

input_dim = 2  # JHGDPBRINDX or trained_index
output_dim = nb_classes = 2 # entered or exited recession
nb_epoch = 20

X_train = X_train.values.reshape(27, input_dim) # array of size 54
X_test = X_test.values.reshape(82, input_dim) # array of size 168
X_train = X_train.astype('float')
X_test = X_test.astype('float')

# Adjust sizes of Y_train and Y_test to match X_train and X_test
Y_train = to_categorical(y_train[27:], nb_classes) # restricting view to recent data
Y_test = to_categorical(y_test[82:], nb_classes) # restricting view to recent data

"""**To begin - determine an ideal batch size to train the ANN model with.**

*Implementing several ANN models with different batch sizes:*
"""

# 27 mini batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=27, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 55 mini batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=55, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 109 mini batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 163 mini batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=163, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 218 mini batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=218, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# full sample as batch size:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer='sgd', loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=X_train.shape[0], epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""**Results of mini batch size testing:**

* After trial and error at approximately median intervals of the increasing batch size, utilizing a batch size of `109` yielded the best test accuracy with this Deep Learning technique.
* Using the full sample as the batch size seems to have maintained the greatest accuracy over the progression between individual epochs, however this batch size did so with more error than a batch size of `109` experienced.
  * A batch size of `109` reached a level of accuracy between epochs that was still close to that of the full sample batch size model (yielding about a 4% difference between the highest accuracies attained by each model).

  * A batch size of the full sample also triggered a great deal of retracing over the second half of epochs in development of the model's layers, which may cause the printed accuracy of individual epochs to be less reliably interpretable than those of all the other ANN models created here.

**Next step: determine an ideal learning rate for the stochastic gradient descent.**

*Implementing several ANN models with different learning rates:*
"""

sgd_1 = optimizers.SGD(lr=1)
sgd_10 = optimizers.SGD(lr=10)
sgd_100 = optimizers.SGD(lr=100)
sgd_1000 = optimizers.SGD(lr=1000)
sgd_10000 = optimizers.SGD(lr=10000)
sgd_100000 = optimizers.SGD(lr=100000)

# 1.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_1, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 10.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_10, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 100.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_100, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 1,000.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_1000, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 10,000.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_10000, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

# 100,000.0 learning rate:

model = Sequential()
# our first dense layer
model.add(Dense(218, input_shape=(2,), activation="relu"))
# our second dense layer
model.add(Dense(109, activation="relu"))
# last layer is the output layer.
model.add(Dense(2, activation="softmax"))

model.compile(optimizer=sgd_100000, loss='categorical_crossentropy',
              metrics=['accuracy'])

# setting verbose=1 prints out some results after each epoch
model.fit(X_train, Y_train, batch_size=109, epochs=20, verbose=1)

score = model.evaluate(X_test, Y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""**Observations of learning rate test results:**

* Test accuracy seems to plateau quickly, even between layers.

* Large amounts of loss over each ANN epoch progression, especially in the primary stages of learning; 
  * ANN with learning rate of 10.0 seems to minimize loss the best.
    * This model also yields the same Test accuracy as most others, however, most likely due to plateauing.

# **Inferences**

Let's break down the most significant features of the most effective models yielded by each method employed in this study:

**Time Series Analysis**
* ARIMA(2,1,2)
  * Greatest log-likelihood.
  * Lowest AIC score.
  * Approximately normal (and periodic) residuals.

**Deep Learning**
* ANN with `batch_size=109` and `sgd_10` learning rate
  * Test accuracy of about 80.5%
  * Least amount of loss (after learning).

Of these two high-achieving models of the dataset, the ARIMA(2,1,2) model performed best overall for this research.
  * In particular, the models produced by ARIMA modeling appear to produce results that not only more closely resemble the data and its element of periodicity, but also do so with less error.

  * In fact, it is especially important to acknowledge that the highest level of accuracy attained by the best-performing ANN model still occurred after the hidden layers in the model experienced an extremely large amount of loss similar to all other ANN models built here. 
    * This did not appear to be an issue nearly as much over the course of the formulation of the various ARIMA models.

*Takeaways and considerations for discussion:*

* Time Series Analysis with ARIMA models yielded greater results, likely due to the univariate relation between time and the target variable (GDP-Based Recession Indicator Index values).
  * Deep Learning with ANN models could yield more comparable, and possibly more statistically significant, results if the data correlated with more variables than time alone.
* Perhaps comparing this GDP-Based Recession Indicator Index with further time-related data describing a variety of other branches within the United States' economy would help predict future periods of economic recession more accurately as well.
  * This effort would maintain consistency across the natures of such datasets with the studied dataset.
"""